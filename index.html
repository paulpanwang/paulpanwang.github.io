<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Panwang Pan</title>
  
  <meta name="author" content="Panwang Pan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="./images/bytedance.jpeg"/>
  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@700&family=Noto+Sans:wght@400;500;600;700&display=swap">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
              <p style="text-align:center">
                <name>Panwang Pan</name>
              </p>
              <p>I am currently employed as a Researcher and Developer at Pico Architecture Group within  <a href="https://www.bytedance.com/en/">ByteDance Ltd</a>. Previously,  I held the position of Senior Algorithm Engineer at  <a href="https://www.alibabacloud.com/zh">Alibaba Cloud</a>, where I specialized in 3D Face Reconstruction and 6DoF Pose Estimation.  
              </p>

              <p>In 2019, I earned my Master's degree from <a href="https://www.xmu.edu.cn/">Xiamen University</a>, where I was enrolled in the prestigious Key Laboratory of Underwater Acoustic Communication, within the School of Informatics.</p>

              <p> My research interests primarily lie at the intersection of <strong>Efficient Deep Learning</strong>(Quantization & Reparameterization) and <strong>3D Computer Vision</strong> (Vision and depth sensor-based 3D reconstruction, understanding and manipulation).                
              </p>
              <p style="text-align:center">
       

                <a href="mailto:panwangpan@stu.xmu.edu.cn">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  <span><strong>Email</strong></span>
                </a> &nbsp/&nbsp
		      
<!--                 <a href="./index.html">
                  <span class="icon"><i class="fa fa-sticky-note"></i></span>
                  <span><strong>CV</strong></span>
                </a> &nbsp/&nbsp -->
		      
                <a href="https://scholar.google.com/citations?hl=en&user=cdfWZZ4AAAAJ">
                  <span class="icon"><i class="ai ai-google-scholar ai-1x"></i></span>
                  <span><strong>Google Scholar</strong></span>
                </a> &nbsp/&nbsp
                <a href="https://github.com/paulpanwang">
                  <span class="icon"><i class="fa fab fa-github"></i></span>
                  <span><strong>Github</strong></span>
                </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="./images/me.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
 
              <p>
		[07/2023] One paper accepted to ICCV 2023. <br>
                [08/2022] Join <a href="https://www.mmlab-ntu.com/">Pico@ByteDance</a>! <br>
	        [07/2019] Join <a href="https://www.alibabacloud.com/zh">Aliyun Cloud@Alibaba</a>! <br>
              </p>
  
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="cas6d_stop()" onmouseover="cas6d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cas6d_video'><img src="images/cas6d_dynamic.png" width="200",height="400"></div>
                <img src='images/cas6d_static.png' width="200">
              </div>
              <script type="text/javascript">
                function cas6d_start() {
                  document.getElementById('cas6d_video').style.opacity = "1";
                }

                function cas6d_stop() {
                  document.getElementById('cas6d_video').style.opacity = "0";
                }
                cas6d_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://github.com/paulpanwang/Cas6D">
                <papertitle>Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable Approach using RGB Images</papertitle>
              </a>
              <br>
              <strong>Panwang Pan<strong>*</strong></strong>,
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan<strong>*</strong></a>,
              <a href="https://brandonyfeng.github.io/">Brandon Y. Feng</a>,
              <a href="https://peihaowang.github.io/">Peihao Wang</a>,
              <a href="https://xggnet.github.io/">Chenxin Li</a>,
              <a href="https://vita-group.github.io/">Zhangyang Wang</a>
              <br>
              <!-- <em>ICCV</em>, 2021 &nbsp <font color="#ff6a5c"><strong>(Oral Presentation)</strong></font> -->
              <br>
							[<a href="https://arxiv.org/abs/2306.07598">Paper</a>]
              [<a href="https://github.com/paulpanwang/Cas6D">Project</a>]
							[<a href="https://github.com/paulpanwang/Cas6D">Code</a>]
              <img src="https://img.shields.io/github/stars/paulpanwang/Gen6DNeRF?style=social">
              <img src="https://img.shields.io/github/forks/paulpanwang/Gen6DNeRF?style=social">
              <p></p>
              <p>By discretizing the pose search range using multiple pose bins and progressively narrowing the pose search range in each stage using predictions from the previous stage, Cas6D can overcome the large gap between pose candidates and ground truth poses, which is a common failure mode in sparse-view scenarios.</p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="POPE_stop()" onmouseover="POPE_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='POPE_video'><img src="images/POPE_static.gif" width="200",height="400"></div>
                <img src='images/POPE.png' width="200">
              </div>
              <script type="text/javascript">
                function POPE_start() {
                  document.getElementById('POPE_video').style.opacity = "1";
                }

                function POPE_stop() {
                  document.getElementById('POPE_video').style.opacity = "0";
                }
                POPE_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://paulpanwang.github.io/POPE/">
                <papertitle>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</papertitle>
              </a>
              <br>
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan<strong>*</strong></a>,
              <strong>Panwang Pan<strong>*</strong></strong>,
              <a href="https://peihaowang.github.io/">Peihao Wang</a>,
              <a href="https://yifanjiang19.github.io/">Yifan Jiang </a>,
              <a href="https://ir1d.github.io/">Dejia Xu</a>,
              <a href="https://hwjiang1510.github.io/">Hanwen Jiang</a>, <br>
              <a href="https://vita-group.github.io/">Zhangyang Wang</a>
              <br>
              <!-- <em>ICCV</em>, 2021 &nbsp <font color="#ff6a5c"><strong>(Oral Presentation)</strong></font> -->
              <br>
							[<a href="https://arxiv.org/abs/2305.15727">Paper</a>]
              [<a href="https://paulpanwang.github.io/POPE/">Project</a>]
							[<a href="https://github.com/paulpanwang/POPE">Code</a>]
              <img src="https://img.shields.io/github/stars/paulpanwang/POPE?style=social">
              <img src="https://img.shields.io/github/forks/paulpanwang/POPE?style=social">
              <p></p>
              <p>We introduce Promptable Object Pose Estimator (POPE), a zero-shot framework for estimating the 6DoF pose of objects in any target image, leveraging a single reference such as a cropped image or a sketch.</p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="adafocus_stop()" onmouseover="adafocus_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='adafocus_video'><img src="images/stega_nerf_dynamic.gif" width="200"></div>
                <img src='images/stega_nerf_static.gif' width="200">
              </div>
              <script type="text/javascript">
                function adafocus_start() {
                  document.getElementById('adafocus_video').style.opacity = "1";
                }

                function adafocus_stop() {
                  document.getElementById('adafocus_video').style.opacity = "0";
                }
                adafocus_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://xggnet.github.io/StegaNeRF/">
                <papertitle>StegaNeRF: Embedding Invisible Information within Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://xggnet.github.io/">Chenxin Li<strong>*</strong></a>,
              <a href="https://brandonyfeng.github.io/">Brandon Y. Feng<strong>*</strong></a>,
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan<strong>*</strong></a>,
              <strong>Panwang Pan</strong>,
              <a href="https://vita-group.github.io/">Zhangyang Wang</a>
              <br>
              <!-- <em>ICCV</em>, 2021 &nbsp <font color="#ff6a5c"><strong>(Oral Presentation)</strong></font> -->
              <br>
							[<a href="https://arxiv.org/abs/2212.01602">Paper</a>]
              [<a href="https://xggnet.github.io/StegaNeRF/">Project</a>]
							[<a href="https://github.com/XGGNet/StegaNeRF">Code</a>]
              <img src="https://img.shields.io/github/stars/XGGNet/StegaNeRF?style=social">
              <img src="https://img.shields.io/github/forks/XGGNet/StegaNeRF?style=social">
              <p></p>
              <p>StegaNeRF achieves reliable recovery of hidden information with minimal impact on the NeRF rendering quality. This work offers a promising outlook on ownership identification in NeRF and calls for more attention and effort on related problems.</p>
            </td>
          </tr>
        </tbody></table>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%">
              <img src="images/bytedance.jpeg" width="30" height="18"> ByteDance Ltd, Beijing, China, Senior Computer Vision Algorithm Engineer.
            </td>
            <td width="25%">08/2022 - Present</td>
          </tr>

          <tr>
            <td width="75%">
              <img src="images/alibaba.png" width="30" height="18"> Alibaba Cloud, Hangzhou, China, Senior Computer Vision Algorithm Engineer <br>
              advised by Yimin Long, Lulu Hu and HongTao Duan.
            </td>
            <td width="25%">07/2019 - 07/2022</td>
          </tr>
   
          <tr>
            <td width="75%">
              <img src="images/nvidia.png" width="30" height="18"> DevTech Compute, NVIDIA, Beijing, China, 
              AI Developer Technology Engineer Intern <br>
              advised by <a href="https://www.zhihu.com/people/lxp121">Xipeng Li </a>.
            </td>
            <td width="25%">07/2018 - 10/2018</td>
          </tr>
        </tbody></table>

      
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Awards</heading> 
          <p>2019, <a href="https://informatics.xmu.edu.cn/info/1034/8061.htm"> <strong> Outstanding Graduates of Xiamen University</strong></a> </p>
          <p>2018, <a href="https://informatics.xmu.edu.cn/info/1025/6566.htm"> <strong> National Scholarship for Postgraduates</strong></a> </p>
          <p>2018, <a href="index.html"> <strong> First Prize of Graduate Electronics Design Contest(GEDC)</strong></a> </p>
	  <p>2018, <a href="index.html"> <strong> Second Prize of <a href="https://cpipc.acge.org.cn/">MCM</a> & <a href="https://cpipc.acge.org.cn/">CPIPC</a> </strong></a> </p>
          <p>2017, <a href="index.html"> <strong> ZhongXian Huang Scholarship of Xiamen University</strong></a> </p>
          <p>2015, <a href="index.html"> <strong> National Scholarship for Undergraduates </strong></a> </p>
          <!-- <p>2015, <a href="https://stu.fjnu.edu.cn/cb/30/c5870a117552/page.htm"> <strong> National Scholarship for Undergraduates </strong></a> </p> -->
        </td>
      </tr>
    </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <p><a href="https://research.facebook.com/blog/2023/4/announcing-the-2023-meta-research-phd-fellowship-award-winners/">Meta Research PhD Fellowship</a>, Finalist, 2023</p>
            <p><a href="https://aisingapore.org/research/aisg-phd-fellowship-programme/">AISG PhD Fellowship</a>, 2021</p>
          </td>
        </tr>
      </tbody></table> -->


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Services</heading>
          <p>Conference Reviewer: CVPR, ICCV, SIGGRAPH, NeurIPS</p>
          <p>Journal Reviewer: TOG, IJCV</p>
        </td>
      </tr>
    </tbody></table> -->
      <!-- </td>
    </tr>
  </table> -->


  <!--           CopyRight-->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
  <p style="text-align:right;">Modified from <a href="https://jonbarron.info/">Jon Barron</a></p>
                </tbody>
            </table>
    
      </td>
    </tr>
  </table>
  
</body>

</html>
